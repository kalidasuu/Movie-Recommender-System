{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalidasuu/Movie-Recommender-System/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Longforner"
      ],
      "metadata": {
        "id": "xEnHz7uX4Wuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from transformers import LongformerModel, LongformerTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "import numpy as np\n",
        "\n",
        "# --- Load Longformer Model and Tokenizer ---\n",
        "print(\"Loading Longformer model and tokenizer...\")\n",
        "model_name = 'allenai/longformer-base-4096'\n",
        "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "model = LongformerModel.from_pretrained(model_name)\n",
        "\n",
        "# --- Set Device (GPU if available, else CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval() # Set model to evaluation mode\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"Longformer model loaded.\")\n",
        "\n",
        "# --- Helper Function for Sentence Embeddings ---\n",
        "def get_sentence_embeddings(text, batch_size=4):\n",
        "    \"\"\"\n",
        "    Splits text into sentences, tokenizes them, and gets Longformer embeddings.\n",
        "    Handles long documents by processing sentences in batches.\n",
        "    Returns:\n",
        "        sentences (list): List of original sentence strings.\n",
        "        sentence_embeddings (np.array): NumPy array of sentence embeddings.\n",
        "    \"\"\"\n",
        "    doc = nlp(text) # nlp is globally defined at the start of the cell\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "    if not sentences:\n",
        "        print(\"Warning: No valid sentences found in the input text.\")\n",
        "        return [], np.array([])\n",
        "\n",
        "    all_sentence_embeddings = []\n",
        "    print(f\"Total sentences to process: {len(sentences)}\")\n",
        "\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        batch_sentences = sentences[i:i + batch_size] # CORRECTED: using batch_size\n",
        "        try:\n",
        "            inputs = tokenizer(\n",
        "                batch_sentences,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=tokenizer.model_max_length\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "            all_sentence_embeddings.extend(cls_embeddings)\n",
        "            # Removed detailed batch print to reduce output clutter unless needed for debugging speed\n",
        "            # print(f\"  Processed batch {i // batch_size + 1}/{(len(sentences) + batch_size - 1) // batch_size}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch of sentences (index {i}-{i+len(batch_sentences)-1}): {e}\")\n",
        "            all_sentence_embeddings.extend([np.zeros(model.config.hidden_size)] * len(batch_sentences))\n",
        "            continue\n",
        "\n",
        "    return sentences, np.array(all_sentence_embeddings)\n",
        "\n",
        "# --- Centroid-Based Summarization Function (Optimized to accept pre-calculated embeddings) ---\n",
        "def centroid_summarization_optimized(sentences, embeddings, num_sentences=3):\n",
        "    \"\"\"\n",
        "    Generates an extractive summary using a centroid-based approach.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Centroid-Based Summarization ---\")\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize.\")\n",
        "        return [], []\n",
        "\n",
        "    if num_sentences <= 0:\n",
        "        print(\"  Number of sentences for summary must be positive.\")\n",
        "        return [], []\n",
        "\n",
        "    num_sentences_to_extract = min(num_sentences, len(sentences))\n",
        "\n",
        "    document_centroid = np.mean(embeddings, axis=0)\n",
        "    similarities = cosine_similarity(embeddings, document_centroid.reshape(1, -1)).flatten()\n",
        "\n",
        "    summary_sentences_mmr = []\n",
        "    selected_indices = set()\n",
        "    ranked_initial_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "    for _ in range(num_sentences_to_extract):\n",
        "        best_sentence_idx = -1\n",
        "        max_mmr_score = -1\n",
        "\n",
        "        for i in ranked_initial_indices:\n",
        "            if i not in selected_indices:\n",
        "                relevance = similarities[i]\n",
        "\n",
        "                if not selected_indices:\n",
        "                    mmr_score = relevance\n",
        "                else:\n",
        "                    diversity_scores = cosine_similarity(embeddings[i].reshape(1, -1),\n",
        "                                                         embeddings[list(selected_indices)])\n",
        "                    redundancy = np.max(diversity_scores)\n",
        "                    lambda_param = 0.7\n",
        "                    mmr_score = lambda_param * relevance - (1 - lambda_param) * redundancy\n",
        "\n",
        "                if mmr_score > max_mmr_score:\n",
        "                    max_mmr_score = mmr_score\n",
        "                    best_sentence_idx = i\n",
        "\n",
        "        if best_sentence_idx != -1:\n",
        "            summary_sentences_mmr.append((sentences[best_sentence_idx], best_sentence_idx))\n",
        "            selected_indices.add(best_sentence_idx)\n",
        "            ranked_initial_indices = ranked_initial_indices[ranked_initial_indices != best_sentence_idx]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    summary_sentences_mmr.sort(key=lambda x: x[1])\n",
        "    final_summary_sents = [s[0] for s in summary_sentences_mmr]\n",
        "    final_summary_indices = [s[1] for s in summary_sentences_mmr]\n",
        "\n",
        "    print(\"--- Centroid-Based Summarization Complete ---\")\n",
        "    return final_summary_sents, final_summary_indices\n",
        "\n",
        "# --- K-Means Based Summarization Function (Optimized to accept pre-calculated embeddings) ---\n",
        "def kmeans_summarization_optimized(sentences, embeddings, num_clusters=5, num_sentences_per_cluster=1):\n",
        "    \"\"\"\n",
        "    Generates an extractive summary using K-Means clustering.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting K-Means Based Summarization ---\")\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize.\")\n",
        "        return [], []\n",
        "\n",
        "    if num_clusters <= 0 or num_sentences_per_cluster <= 0:\n",
        "        print(\"  Number of clusters and sentences per cluster must be positive.\")\n",
        "        return [], []\n",
        "\n",
        "    effective_num_clusters = min(num_clusters, len(sentences))\n",
        "\n",
        "    if effective_num_clusters == 0:\n",
        "        print(\"  Not enough sentences to form clusters.\")\n",
        "        return [], []\n",
        "\n",
        "    kmeans = KMeans(n_clusters=effective_num_clusters, random_state=42, n_init='auto')\n",
        "    kmeans.fit(embeddings)\n",
        "    clusters = kmeans.labels_\n",
        "    centroids = kmeans.cluster_centers_\n",
        "\n",
        "    summary_sentences_with_idx = []\n",
        "    selected_indices = set()\n",
        "\n",
        "    for i in range(effective_num_clusters):\n",
        "        cluster_sentence_indices = np.where(clusters == i)[0]\n",
        "\n",
        "        if len(cluster_sentence_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        distances = cdist(embeddings[cluster_sentence_indices], centroids[i].reshape(1, -1), 'cosine').flatten()\n",
        "        sorted_cluster_indices = cluster_sentence_indices[np.argsort(distances)]\n",
        "\n",
        "        count_selected_from_cluster = 0\n",
        "        for original_idx in sorted_cluster_indices:\n",
        "            if original_idx not in selected_indices:\n",
        "                summary_sentences_with_idx.append((sentences[original_idx], original_idx))\n",
        "                selected_indices.add(original_idx)\n",
        "                count_selected_from_cluster += 1\n",
        "                if count_selected_from_cluster >= num_sentences_per_cluster:\n",
        "                    break\n",
        "\n",
        "    summary_sentences_with_idx.sort(key=lambda x: x[1])\n",
        "    final_summary_sents = [s[0] for s in summary_sentences_with_idx]\n",
        "    final_summary_indices = [s[1] for s in summary_sentences_with_idx]\n",
        "\n",
        "    print(\"--- K-Means Based Summarization Complete ---\")\n",
        "    return final_summary_sents, final_summary_indices\n",
        "\n",
        "# --- Combined Extractive Summarization Function (Optimized to accept pre-calculated embeddings) ---\n",
        "def combined_extractive_summary_optimized(sentences, embeddings, total_summary_sentences=7,\n",
        "                                centroid_sentences_to_propose=5,\n",
        "                                kmeans_clusters_to_propose=4,\n",
        "                                kmeans_sentences_per_cluster_to_propose=1,\n",
        "                                lambda_param_mmr=0.7):\n",
        "    \"\"\"\n",
        "    Generates a single extractive summary by combining candidates from\n",
        "    both centroid-based and K-Means approaches, then using MMR for final selection.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Combined Extractive Summarization ---\")\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize combined.\")\n",
        "        return []\n",
        "\n",
        "    centroid_candidates_sents, centroid_candidates_indices = centroid_summarization_optimized(\n",
        "        sentences, embeddings, num_sentences=centroid_sentences_to_propose\n",
        "    )\n",
        "    print(f\"  Centroid proposed {len(centroid_candidates_sents)} candidates.\")\n",
        "\n",
        "    kmeans_candidates_sents, kmeans_candidates_indices = kmeans_summarization_optimized(\n",
        "        sentences, embeddings, num_clusters=kmeans_clusters_to_propose, num_sentences_per_cluster=kmeans_sentences_per_cluster_to_propose\n",
        "    )\n",
        "    print(f\"  K-Means proposed {len(kmeans_candidates_sents)} candidates.\")\n",
        "\n",
        "    # Combine candidates and their original indices, removing duplicates\n",
        "    combined_candidates_map = {}\n",
        "    for idx, sent in zip(centroid_candidates_indices, centroid_candidates_sents):\n",
        "        combined_candidates_map[idx] = sent\n",
        "    for idx, sent in zip(kmeans_candidates_indices, kmeans_candidates_sents):\n",
        "        combined_candidates_map[idx] = sent\n",
        "\n",
        "    all_candidate_indices_sorted = sorted(combined_candidates_map.keys())\n",
        "    all_candidate_sentences = [combined_candidates_map[idx] for idx in all_candidate_indices_sorted]\n",
        "    all_candidate_embeddings = np.array([embeddings[idx] for idx in all_candidate_indices_sorted])\n",
        "\n",
        "    if not all_candidate_sentences or all_candidate_embeddings.shape[0] == 0:\n",
        "        print(\"  No unique candidates found after combining. Cannot generate combined summary.\")\n",
        "        return []\n",
        "\n",
        "    num_sentences_to_extract = min(total_summary_sentences, len(all_candidate_sentences))\n",
        "    print(f\"  Total unique candidates: {len(all_candidate_sentences)}. Extracting {num_sentences_to_extract} for combined summary.\")\n",
        "\n",
        "    document_centroid = np.mean(embeddings, axis=0)\n",
        "    candidate_similarities = cosine_similarity(all_candidate_embeddings, document_centroid.reshape(1, -1)).flatten()\n",
        "\n",
        "    final_summary_sentences = []\n",
        "    selected_candidate_indices = set()\n",
        "\n",
        "    ranked_initial_candidate_indices = np.argsort(candidate_similarities)[::-1]\n",
        "\n",
        "    for _ in range(num_sentences_to_extract):\n",
        "        best_idx_in_candidates = -1\n",
        "        max_mmr_score = -1\n",
        "\n",
        "        for i_candidate in ranked_initial_candidate_indices:\n",
        "            if i_candidate not in selected_candidate_indices:\n",
        "                relevance = candidate_similarities[i_candidate]\n",
        "\n",
        "                if not selected_candidate_indices:\n",
        "                    mmr_score = relevance\n",
        "                else:\n",
        "                    diversity_scores = cosine_similarity(all_candidate_embeddings[i_candidate].reshape(1, -1),\n",
        "                                                         all_candidate_embeddings[list(selected_candidate_indices)])\n",
        "                    redundancy = np.max(diversity_scores)\n",
        "\n",
        "                    mmr_score = lambda_param_mmr * relevance - (1 - lambda_param_mmr) * redundancy\n",
        "\n",
        "                if mmr_score > max_mmr_score:\n",
        "                    max_mmr_score = mmr_score\n",
        "                    best_idx_in_candidates = i_candidate\n",
        "\n",
        "        if best_idx_in_candidates != -1:\n",
        "            final_summary_sentences.append((all_candidate_sentences[best_idx_in_candidates],\n",
        "                                             all_candidate_indices_sorted[best_idx_in_candidates]))\n",
        "            selected_candidate_indices.add(best_idx_in_candidates)\n",
        "            ranked_initial_candidate_indices = ranked_initial_candidate_indices[ranked_initial_candidate_indices != best_idx_in_candidates]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    final_summary_sentences.sort(key=lambda x: x[1])\n",
        "    final_summary = [s[0] for s in final_summary_sentences]\n",
        "\n",
        "    print(\"--- Combined Extractive Summarization Complete ---\")\n",
        "    return final_summary\n",
        "\n",
        "# --- Example Usage and Testing ---\n",
        "long_document = \"\"\"\n",
        "Artificial intelligence (AI) has rapidly transformed various sectors, revolutionizing industries from healthcare to finance. In healthcare, AI assists in diagnosing diseases earlier and more accurately, personalizing treatment plans, and accelerating drug discovery. Machine learning algorithms, a subset of AI, analyze vast amounts of patient data to identify patterns that human doctors might miss, leading to more effective interventions. For instance, AI-powered tools can detect subtle signs of retinopathy from eye scans, potentially preventing blindness. The integration of AI into electronic health records is also streamlining administrative tasks, freeing up medical professionals to focus more on patient care. This technological leap promises to enhance diagnostic capabilities and optimize treatment protocols significantly.\n",
        "\n",
        "The financial industry also heavily leverages AI for fraud detection, algorithmic trading, and personalized financial advice. AI systems can monitor transactions in real-time, identifying unusual patterns indicative of fraudulent activity with high precision. Furthermore, robo-advisors powered by AI provide automated, data-driven investment advice tailored to individual risk tolerance and financial goals, making financial planning more accessible to a wider demographic. The use of AI in predicting market trends and managing portfolios is becoming increasingly sophisticated, offering new avenues for investors.\n",
        "\n",
        "Beyond these, AI is deeply embedded in everyday life through virtual assistants like Siri and Alexa, recommendation engines on streaming platforms, and autonomous vehicles. AI's role in natural language processing (NLP) has led to advancements in language translation and sentiment analysis, impacting global communication and customer service. The ethical implications of AI, however, are a growing concern among researchers and policymakers. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation. Ensuring transparency, fairness, and accountability in AI development is paramount to harnessing its benefits responsibly.\n",
        "\n",
        "Research in AI continues to advance at an astonishing pace, focusing on areas like explainable AI (XAI) to make AI decisions more understandable, and robust AI to improve performance in real-world, unpredictable environments. Novel architectures like generative adversarial networks (GANs) and reinforcement learning are pushing the boundaries of what AI can achieve, from creating realistic imagery to mastering complex games. The future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence (AGI) and enhanced human-computer interaction, leading to smarter cities and more efficient resource management. However, achieving these advancements responsibly will necessitate ongoing collaboration between technologists, policymakers, and ethicists to address the complex challenges that arise. The rapid pace of development means that continuous public discourse and legislative adaptation are critical to navigate the challenges and maximize the societal benefits of AI, ensuring it serves humanity's best interests.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original Document Length (sentences):\", sum(1 for _ in nlp(long_document).sents))\n",
        "\n",
        "# --- OPTIMIZATION: Calculate document embeddings only ONCE ---\n",
        "print(\"\\nCalculating document embeddings (this might take a while for long texts)...\")\n",
        "sentences_list, embeddings_array = get_sentence_embeddings(long_document, batch_size=8)\n",
        "print(\"Embeddings calculation complete.\")\n",
        "\n",
        "\n",
        "# --- Individual Centroid-Based Summarization ---\n",
        "'''print(\"\\n\" + \"=\"*80)\n",
        "print(\"Individual Centroid-Based Summary:\")\n",
        "centroid_summary, _ = centroid_summarization_optimized(sentences_list, embeddings_array, num_sentences=5)\n",
        "for i, sent in enumerate(centroid_summary):\n",
        "    print(f\"{i+1}. {sent}\")\n",
        "\n",
        "\n",
        "# --- Individual K-Means Based Summarization ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Individual K-Means Based Summary:\")\n",
        "kmeans_summary, _ = kmeans_summarization_optimized(sentences_list, embeddings_array, num_clusters=4, num_sentences_per_cluster=1)\n",
        "for i, sent in enumerate(kmeans_summary):\n",
        "    print(f\"{i+1}. {sent}\")'''\n",
        "\n",
        "\n",
        "# --- Combined Extractive Summarization ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Combined Extractive Summary:\")\n",
        "combined_summary = combined_extractive_summary_optimized(\n",
        "    sentences_list,\n",
        "    embeddings_array,\n",
        "    total_summary_sentences=6,\n",
        "    centroid_sentences_to_propose=7,\n",
        "    kmeans_clusters_to_propose=5,\n",
        "    kmeans_sentences_per_cluster_to_propose=1\n",
        ")\n",
        "for i, sent in enumerate(combined_summary):\n",
        "    print(f\"{i+1}. {sent}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nAll summarization processes complete.\")"
      ],
      "metadata": {
        "id": "E9WZGQZ-5Lx5",
        "outputId": "ee3d1366-03ef-4b7c-c60f-ff46bdee229c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Longformer model and tokenizer...\n",
            "Using device: cpu\n",
            "Longformer model loaded.\n",
            "Original Document Length (sentences): 20\n",
            "\n",
            "Calculating document embeddings (this might take a while for long texts)...\n",
            "Total sentences to process: 20\n",
            "Embeddings calculation complete.\n",
            "\n",
            "================================================================================\n",
            "Combined Extractive Summary:\n",
            "\n",
            "--- Starting Combined Extractive Summarization ---\n",
            "\n",
            "--- Starting Centroid-Based Summarization ---\n",
            "--- Centroid-Based Summarization Complete ---\n",
            "  Centroid proposed 7 candidates.\n",
            "\n",
            "--- Starting K-Means Based Summarization ---\n",
            "--- K-Means Based Summarization Complete ---\n",
            "  K-Means proposed 5 candidates.\n",
            "  Total unique candidates: 11. Extracting 6 for combined summary.\n",
            "--- Combined Extractive Summarization Complete ---\n",
            "1. Artificial intelligence (AI) has rapidly transformed various sectors, revolutionizing industries from healthcare to finance.\n",
            "2. The integration of AI into electronic health records is also streamlining administrative tasks, freeing up medical professionals to focus more on patient care.\n",
            "3. Furthermore, robo-advisors powered by AI provide automated, data-driven investment advice tailored to individual risk tolerance and financial goals, making financial planning more accessible to a wider demographic.\n",
            "4. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation.\n",
            "5. The future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence (AGI) and enhanced human-computer interaction, leading to smarter cities and more efficient resource management.\n",
            "6. The rapid pace of development means that continuous public discourse and legislative adaptation are critical to navigate the challenges and maximize the societal benefits of AI, ensuring it serves humanity's best interests.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "All summarization processes complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BART"
      ],
      "metadata": {
        "id": "_s3CRcmh5ieu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries (only runs if not already installed)\n",
        "#%pip install transformers torch\n",
        "\n",
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# --- Load BART Model and Tokenizer ---\n",
        "print(\"Loading BART model and tokenizer for abstractive summarization...\")\n",
        "bart_model_name = 'facebook/bart-large-cnn' # This is a good choice for summarization\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
        "\n",
        "# --- Set Device (GPU if available, else CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "bart_model.eval() # Set to evaluation mode\n",
        "bart_model.to(device)\n",
        "print(f\"BART using device: {device}\")\n",
        "print(\"BART model loaded.\")\n",
        "\n",
        "# --- Abstractive Summarization Function (using BART) ---\n",
        "def bart_abstractive_summary(text_to_summarize, max_length=150, min_length=50, num_beams=4, early_stopping=True):\n",
        "    \"\"\"\n",
        "    Generates an abstractive summary using the pre-loaded BART model.\n",
        "    Assumes bart_tokenizer and bart_model are loaded globally.\n",
        "\n",
        "    Args:\n",
        "        text_to_summarize (str or list of str): The input text (or list of sentences) to summarize.\n",
        "                                                  If a list, it will be joined into a single string.\n",
        "        max_length (int): Maximum length of the generated summary.\n",
        "        min_length (int): Minimum length of the generated summary.\n",
        "        num_beams (int): Number of beams for beam search. Higher values lead to better quality but slower generation.\n",
        "        early_stopping (bool): Whether to stop beam search when all beams have finished their generation.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated abstractive summary.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting BART Abstractive Summarization ---\")\n",
        "\n",
        "    if isinstance(text_to_summarize, list):\n",
        "        text_to_summarize = \" \".join(text_to_summarize)\n",
        "\n",
        "    if not text_to_summarize.strip():\n",
        "        print(\"  Input text for abstractive summary is empty. Cannot summarize.\")\n",
        "        return \"\"\n",
        "\n",
        "    inputs = bart_tokenizer( # bart_tokenizer is now accessible globally\n",
        "        [text_to_summarize],\n",
        "        max_length=1024, # BART's typical max input length\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    ).to(device) # device is also globally accessible\n",
        "\n",
        "    summary_ids = bart_model.generate( # bart_model is now accessible globally\n",
        "        inputs[\"input_ids\"],\n",
        "        num_beams=num_beams,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        early_stopping=early_stopping\n",
        "    )\n",
        "\n",
        "    summary_text = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"--- BART Abstractive Summarization Complete ---\")\n",
        "    return summary_text\n",
        "\n",
        "# --- Example Usage for BART only ---\n",
        "input_text_for_bart = \" \".join(combined_summary)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Abstractive Summary (using BART directly on the full text):\")\n",
        "bart_only_summary = bart_abstractive_summary(\n",
        "    input_text_for_bart,\n",
        "    max_length=150, # Max length of the final abstractive summary\n",
        "    min_length=50,  # Min length of the final abstractive summary\n",
        "    num_beams=4     # Beam search parameter for quality\n",
        ")\n",
        "print(bart_only_summary)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nBART only summarization complete.\")"
      ],
      "metadata": {
        "id": "61J6V9ac5qhw",
        "outputId": "deb4e767-0f63-40f2-9c98-8bab5e72dad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BART model and tokenizer for abstractive summarization...\n",
            "BART using device: cpu\n",
            "BART model loaded.\n",
            "\n",
            "================================================================================\n",
            "Abstractive Summary (using BART directly on the full text):\n",
            "\n",
            "--- Starting BART Abstractive Summarization ---\n",
            "--- BART Abstractive Summarization Complete ---\n",
            "Artificial intelligence (AI) has rapidly transformed various sectors. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation. Future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "BART only summarization complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Full code for Longformer and BART and pipeline"
      ],
      "metadata": {
        "id": "RDWlIx9jB3tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Setup, Model Loading, and Function Definitions\n",
        "\n",
        "# Install necessary libraries (only runs if not already installed)\n",
        "%pip install transformers torch scikit-learn numpy scipy spacy\n",
        "\n",
        "# Download spaCy model (only downloads if not already present)\n",
        "try:\n",
        "    import spacy\n",
        "    # Try to load the model directly without 'download' first\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy 'en_core_web_sm' model already loaded.\")\n",
        "except OSError:\n",
        "    print(\"spaCy model 'en_core_web_sm' not found. Downloading...\")\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy 'en_core_web_sm' model downloaded and loaded.\")\n",
        "\n",
        "import torch\n",
        "from transformers import LongformerModel, LongformerTokenizer, BartForConditionalGeneration, BartTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "import numpy as np\n",
        "\n",
        "# --- Load Longformer Model and Tokenizer (for Extractive) ---\n",
        "print(\"Loading Longformer model and tokenizer...\")\n",
        "longformer_model_name = 'allenai/longformer-base-4096'\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained(longformer_model_name)\n",
        "longformer_model = LongformerModel.from_pretrained(longformer_model_name)\n",
        "\n",
        "# --- Load BART Model and Tokenizer (for Abstractive) ---\n",
        "print(\"Loading BART model and tokenizer for abstractive summarization...\")\n",
        "bart_model_name = 'facebook/bart-large-cnn' # This is a good choice for summarization\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
        "\n",
        "\n",
        "# --- Set Device (GPU if available, else CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "longformer_model.eval() # Set to evaluation mode\n",
        "longformer_model.to(device)\n",
        "print(f\"Longformer using device: {device}\")\n",
        "\n",
        "bart_model.eval() # Set to evaluation mode\n",
        "bart_model.to(device)\n",
        "print(f\"BART using device: {device}\")\n",
        "print(\"All models loaded and moved to device.\")\n",
        "\n",
        "# --- Helper Function for Sentence Embeddings (Longformer) ---\n",
        "def get_sentence_embeddings(text, batch_size=4):\n",
        "    \"\"\"\n",
        "    Splits text into sentences, tokenizes them, and gets Longformer embeddings.\n",
        "    Handles long documents by processing sentences in batches.\n",
        "    Returns:\n",
        "        sentences (list): List of original sentence strings.\n",
        "        sentence_embeddings (np.array): NumPy array of sentence embeddings.\n",
        "    \"\"\"\n",
        "    # nlp, longformer_tokenizer, longformer_model, and device are global here\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "    if not sentences:\n",
        "        print(\"Warning: No valid sentences found in the input text.\")\n",
        "        return [], np.array([])\n",
        "\n",
        "    all_sentence_embeddings = []\n",
        "    print(f\"Total sentences to process: {len(sentences)}\")\n",
        "\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        batch_sentences = sentences[i:i + batch_size]\n",
        "        try:\n",
        "            inputs = longformer_tokenizer(\n",
        "                batch_sentences,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=longformer_tokenizer.model_max_length\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = longformer_model(**inputs)\n",
        "\n",
        "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "            all_sentence_embeddings.extend(cls_embeddings)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch of sentences (index {i}-{i+len(batch_sentences)-1}): {e}\")\n",
        "            all_sentence_embeddings.extend([np.zeros(longformer_model.config.hidden_size)] * len(batch_sentences))\n",
        "            continue\n",
        "\n",
        "    return sentences, np.array(all_sentence_embeddings)\n",
        "\n",
        "# --- Centroid-Based Summarization Function (Optimized to accept pre-calculated embeddings) ---\n",
        "def centroid_summarization_optimized(sentences, embeddings, num_sentences=3):\n",
        "    \"\"\"\n",
        "    Generates an extractive summary using a centroid-based approach.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize.\")\n",
        "        return [], []\n",
        "\n",
        "    if num_sentences <= 0:\n",
        "        print(\"  Number of sentences for summary must be positive.\")\n",
        "        return [], []\n",
        "\n",
        "    num_sentences_to_extract = min(num_sentences, len(sentences))\n",
        "\n",
        "    document_centroid = np.mean(embeddings, axis=0)\n",
        "    similarities = cosine_similarity(embeddings, document_centroid.reshape(1, -1)).flatten()\n",
        "\n",
        "    summary_sentences_mmr = []\n",
        "    selected_indices = set() # Correctly initialized here\n",
        "    ranked_initial_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "    for _ in range(num_sentences_to_extract):\n",
        "        best_sentence_idx = -1\n",
        "        max_mmr_score = -1\n",
        "\n",
        "        for i in ranked_initial_indices:\n",
        "            if i not in selected_indices:\n",
        "                relevance = similarities[i]\n",
        "\n",
        "                if not selected_indices: # This check is now safe\n",
        "                    mmr_score = relevance\n",
        "                else:\n",
        "                    diversity_scores = cosine_similarity(embeddings[i].reshape(1, -1),\n",
        "                                                         embeddings[list(selected_indices)])\n",
        "                    redundancy = np.max(diversity_scores)\n",
        "                    lambda_param = 0.7\n",
        "                    mmr_score = lambda_param * relevance - (1 - lambda_param) * redundancy\n",
        "\n",
        "                if mmr_score > max_mmr_score:\n",
        "                    max_mmr_score = mmr_score\n",
        "                    best_sentence_idx = i\n",
        "\n",
        "        if best_sentence_idx != -1:\n",
        "            summary_sentences_mmr.append((sentences[best_sentence_idx], best_sentence_idx))\n",
        "            selected_indices.add(best_sentence_idx)\n",
        "            ranked_initial_indices = ranked_initial_indices[ranked_initial_indices != best_sentence_idx]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    summary_sentences_mmr.sort(key=lambda x: x[1])\n",
        "    final_summary_sents = [s[0] for s in summary_sentences_mmr]\n",
        "    final_summary_indices = [s[1] for s in summary_sentences_mmr]\n",
        "\n",
        "    return final_summary_sents, final_summary_indices\n",
        "\n",
        "# --- K-Means Based Summarization Function (Optimized to accept pre-calculated embeddings) ---\n",
        "def kmeans_summarization_optimized(sentences, embeddings, num_clusters=5, num_sentences_per_cluster=1):\n",
        "    \"\"\"\n",
        "    Generates an extractive summary using K-Means clustering.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize.\")\n",
        "        return [], []\n",
        "\n",
        "    if num_clusters <= 0 or num_sentences_per_cluster <= 0:\n",
        "        print(\"  Number of clusters and sentences per cluster must be positive.\")\n",
        "        return [], []\n",
        "\n",
        "    effective_num_clusters = min(num_clusters, len(sentences))\n",
        "\n",
        "    if effective_num_clusters == 0:\n",
        "        print(\"  Not enough sentences to form clusters.\")\n",
        "        return [], []\n",
        "\n",
        "    kmeans = KMeans(n_clusters=effective_num_clusters, random_state=42, n_init='auto')\n",
        "    kmeans.fit(embeddings)\n",
        "    clusters = kmeans.labels_\n",
        "    centroids = kmeans.cluster_centers_\n",
        "\n",
        "    summary_sentences_with_idx = []\n",
        "    selected_indices = set()\n",
        "\n",
        "    for i in range(effective_num_clusters):\n",
        "        cluster_sentence_indices = np.where(clusters == i)[0]\n",
        "\n",
        "        if len(cluster_sentence_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        distances = cdist(embeddings[cluster_sentence_indices], centroids[i].reshape(1, -1), 'cosine').flatten()\n",
        "        sorted_cluster_indices = cluster_sentence_indices[np.argsort(distances)]\n",
        "\n",
        "        count_selected_from_cluster = 0\n",
        "        for original_idx in sorted_cluster_indices:\n",
        "            if original_idx not in selected_indices:\n",
        "                summary_sentences_with_idx.append((sentences[original_idx], original_idx))\n",
        "                selected_indices.add(original_idx)\n",
        "                count_selected_from_cluster += 1\n",
        "                if count_selected_from_cluster >= num_sentences_per_cluster:\n",
        "                    break\n",
        "\n",
        "    summary_sentences_with_idx.sort(key=lambda x: x[1])\n",
        "    final_summary_sents = [s[0] for s in summary_sentences_with_idx]\n",
        "    final_summary_indices = [s[1] for s in summary_sentences_with_idx]\n",
        "\n",
        "    return final_summary_sents, final_summary_indices\n",
        "\n",
        "# --- Combined Extractive Summarization Function (Optimized) ---\n",
        "def combined_extractive_summary_optimized(sentences, embeddings, total_summary_sentences=7,\n",
        "                                centroid_sentences_to_propose=5,\n",
        "                                kmeans_clusters_to_propose=4,\n",
        "                                kmeans_sentences_per_cluster_to_propose=1,\n",
        "                                lambda_param_mmr=0.7):\n",
        "    \"\"\"\n",
        "    Generates a single extractive summary by combining candidates from\n",
        "    both centroid-based and K-Means approaches, then using MMR for final selection.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Combined Extractive Summarization Candidate Generation ---\")\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize combined.\")\n",
        "        return []\n",
        "\n",
        "    centroid_candidates_sents, centroid_candidates_indices = centroid_summarization_optimized(\n",
        "        sentences, embeddings, num_sentences=centroid_sentences_to_propose\n",
        "    )\n",
        "    print(f\"  Centroid proposed {len(centroid_candidates_sents)} candidates.\")\n",
        "\n",
        "    kmeans_candidates_sents, kmeans_candidates_indices = kmeans_summarization_optimized(\n",
        "        sentences, embeddings, num_clusters=kmeans_clusters_to_propose, num_sentences_per_cluster=kmeans_sentences_per_cluster_to_propose\n",
        "    )\n",
        "    print(f\"  K-Means proposed {len(kmeans_candidates_sents)} candidates.\")\n",
        "\n",
        "    # Combine candidates and their original indices, removing duplicates\n",
        "    combined_candidates_map = {}\n",
        "    for idx, sent in zip(centroid_candidates_indices, centroid_candidates_sents):\n",
        "        combined_candidates_map[idx] = sent\n",
        "    for idx, sent in zip(kmeans_candidates_indices, kmeans_candidates_sents):\n",
        "        combined_candidates_map[idx] = sent\n",
        "\n",
        "    all_candidate_indices_sorted = sorted(combined_candidates_map.keys())\n",
        "    all_candidate_sentences = [combined_candidates_map[idx] for idx in all_candidate_indices_sorted]\n",
        "    all_candidate_embeddings = np.array([embeddings[idx] for idx in all_candidate_indices_sorted])\n",
        "\n",
        "    if not all_candidate_sentences or all_candidate_embeddings.shape[0] == 0:\n",
        "        print(\"  No unique candidates found after combining. Cannot generate combined summary.\")\n",
        "        return []\n",
        "\n",
        "    num_sentences_to_extract = min(total_summary_sentences, len(all_candidate_sentences))\n",
        "    print(f\"  Total unique candidates: {len(all_candidate_sentences)}. Extracting {num_sentences_to_extract} for combined summary.\")\n",
        "\n",
        "    document_centroid = np.mean(embeddings, axis=0)\n",
        "    candidate_similarities = cosine_similarity(all_candidate_embeddings, document_centroid.reshape(1, -1)).flatten()\n",
        "\n",
        "    final_summary_sentences = []\n",
        "    selected_candidate_indices = set() # <-- FIXED: Initialized here\n",
        "\n",
        "    ranked_initial_candidate_indices = np.argsort(candidate_similarities)[::-1]\n",
        "\n",
        "    for _ in range(num_sentences_to_extract):\n",
        "        best_idx_in_candidates = -1\n",
        "        max_mmr_score = -1\n",
        "\n",
        "        for i_candidate in ranked_initial_candidate_indices:\n",
        "            if i_candidate not in selected_candidate_indices:\n",
        "                relevance = candidate_similarities[i_candidate]\n",
        "\n",
        "                if not selected_candidate_indices:\n",
        "                    mmr_score = relevance\n",
        "                else:\n",
        "                    diversity_scores = cosine_similarity(all_candidate_embeddings[i_candidate].reshape(1, -1),\n",
        "                                                         all_candidate_embeddings[list(selected_candidate_indices)])\n",
        "                    redundancy = np.max(diversity_scores)\n",
        "\n",
        "                    mmr_score = lambda_param_mmr * relevance - (1 - lambda_param_mmr) * redundancy\n",
        "\n",
        "                if mmr_score > max_mmr_score:\n",
        "                    max_mmr_score = mmr_score\n",
        "                    best_idx_in_candidates = i_candidate\n",
        "\n",
        "        if best_idx_in_candidates != -1:\n",
        "            final_summary_sentences.append((all_candidate_sentences[best_idx_in_candidates],\n",
        "                                             all_candidate_indices_sorted[best_idx_in_candidates]))\n",
        "            selected_candidate_indices.add(best_idx_in_candidates)\n",
        "            ranked_initial_candidate_indices = ranked_initial_candidate_indices[ranked_initial_candidate_indices != best_idx_in_candidates]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    final_summary_sentences.sort(key=lambda x: x[1])\n",
        "    final_summary = [s[0] for s in final_summary_sentences]\n",
        "\n",
        "    print(\"--- Combined Extractive Summarization Selection Complete ---\")\n",
        "    return final_summary\n",
        "\n",
        "# --- Abstractive Summarization Function (using BART) ---\n",
        "def bart_abstractive_summary(text_to_summarize, max_length=150, min_length=50, num_beams=4, early_stopping=True):\n",
        "    \"\"\"\n",
        "    Generates an abstractive summary using the pre-loaded BART model.\n",
        "    Assumes bart_tokenizer and bart_model are loaded globally.\n",
        "\n",
        "    Args:\n",
        "        text_to_summarize (str or list of str): The input text (or list of sentences) to summarize.\n",
        "                                                  If a list, it will be joined into a single string.\n",
        "        max_length (int): Maximum length of the generated summary.\n",
        "        min_length (int): Minimum length of the generated summary.\n",
        "        num_beams (int): Number of beams for beam search. Higher values lead to better quality but slower generation.\n",
        "        early_stopping (bool): Whether to stop beam search when all beams have finished their generation.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated abstractive summary.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting BART Abstractive Summarization ---\")\n",
        "\n",
        "    if isinstance(text_to_summarize, list):\n",
        "        text_to_summarize = \" \".join(text_to_summarize)\n",
        "\n",
        "    if not text_to_summarize.strip():\n",
        "        print(\"  Input text for abstractive summary is empty. Cannot summarize.\")\n",
        "        return \"\"\n",
        "\n",
        "    inputs = bart_tokenizer(\n",
        "        [text_to_summarize],\n",
        "        max_length=1024, # BART's typical max input length\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    ).to(device)\n",
        "\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        num_beams=num_beams,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        early_stopping=early_stopping\n",
        "    )\n",
        "\n",
        "    summary_text = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"--- BART Abstractive Summarization Complete ---\")\n",
        "    return summary_text"
      ],
      "metadata": {
        "id": "nEhKmnvf7L58",
        "outputId": "f3888790-a57b-468b-ddf7-29a2ab2ddaad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.16.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "spaCy 'en_core_web_sm' model already loaded.\n",
            "Loading Longformer model and tokenizer...\n",
            "Loading BART model and tokenizer for abstractive summarization...\n",
            "Longformer using device: cpu\n",
            "BART using device: cpu\n",
            "All models loaded and moved to device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Your Document Text\n",
        "\n",
        "long_document = \"\"\"\n",
        "Artificial intelligence (AI) has rapidly transformed various sectors, revolutionizing industries from healthcare to finance. In healthcare, AI assists in diagnosing diseases earlier and more accurately, personalizing treatment plans, and accelerating drug discovery. Machine learning algorithms, a subset of AI, analyze vast amounts of patient data to identify patterns that human doctors might miss, leading to more effective interventions. For instance, AI-powered tools can detect subtle signs of retinopathy from eye scans, potentially preventing blindness. The integration of AI into electronic health records is also streamlining administrative tasks, freeing up medical professionals to focus more on patient care. This technological leap promises to enhance diagnostic capabilities and optimize treatment protocols significantly.\n",
        "\n",
        "The financial industry also heavily leverages AI for fraud detection, algorithmic trading, and personalized financial advice. AI systems can monitor transactions in real-time, identifying unusual patterns indicative of fraudulent activity with high precision. Furthermore, robo-advisors powered by AI provide automated, data-driven investment advice tailored to individual risk tolerance and financial goals, making financial planning more accessible to a wider demographic. The use of AI in predicting market trends and managing portfolios is becoming increasingly sophisticated, offering new avenues for investors.\n",
        "\n",
        "Beyond these, AI is deeply embedded in everyday life through virtual assistants like Siri and Alexa, recommendation engines on streaming platforms, and autonomous vehicles. AI's role in natural language processing (NLP) has led to advancements in language translation and sentiment analysis, impacting global communication and customer service. The ethical implications of AI, however, are a growing concern among researchers and policymakers. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation. Ensuring transparency, fairness, and accountability in AI development is paramount to harnessing its benefits responsibly.\n",
        "\n",
        "Research in AI continues to advance at an astonishing pace, focusing on areas like explainable AI (XAI) to make AI decisions more understandable, and robust AI to improve performance in real-world, unpredictable environments. Novel architectures like generative adversarial networks (GANs) and reinforcement learning are pushing the boundaries of what AI can achieve, from creating realistic imagery to mastering complex games. The future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence (AGI) and enhanced human-computer interaction, leading to smarter cities and more efficient resource management. However, achieving these advancements responsibly will necessitate ongoing collaboration between technologists, policymakers, and ethicists to address the complex challenges that arise. The rapid pace of development means that continuous public discourse and legislative adaptation are critical to navigate the challenges and maximize the societal benefits of AI, ensuring it serves humanity's best interests.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DF25_IWL_8_f"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: The Summarization Pipeline\n",
        "\n",
        "print(\"--- Starting Hybrid Summarization Pipeline ---\")\n",
        "print(\"Original Document Length (sentences):\", sum(1 for _ in nlp(long_document).sents))\n",
        "\n",
        "\n",
        "# Step 1: Generate Sentence Embeddings using Longformer\n",
        "print(\"\\n[Pipeline Step 1/3] Calculating document embeddings with Longformer...\")\n",
        "sentences_list, embeddings_array = get_sentence_embeddings(long_document, batch_size=8)\n",
        "print(\"  Embeddings calculation complete.\")\n",
        "\n",
        "\n",
        "# Step 2: Generate Combined Extractive Summary\n",
        "print(\"\\n[Pipeline Step 2/3] Generating combined extractive summary...\")\n",
        "combined_extractive_summary_sentences = combined_extractive_summary_optimized(\n",
        "    sentences_list,\n",
        "    embeddings_array,\n",
        "    total_summary_sentences=6, # Desired length for the extractive part\n",
        "    centroid_sentences_to_propose=7,\n",
        "    kmeans_clusters_to_propose=5,\n",
        "    kmeans_sentences_per_cluster_to_propose=1\n",
        ")\n",
        "print(f\"  Extracted {len(combined_extractive_summary_sentences)} sentences.\")\n",
        "print(\"\\nExtractive Summary:\")\n",
        "for i, sent in enumerate(combined_extractive_summary_sentences):\n",
        "    print(f\"{i+1}. {sent}\")\n",
        "\n",
        "\n",
        "# Step 3: Generate Abstractive Summary from Extractive Output using BART\n",
        "print(\"\\n[Pipeline Step 3/3] Generating abstractive summary with BART...\")\n",
        "extractive_text_for_abstractive = \" \".join(combined_extractive_summary_sentences)\n",
        "final_abstractive_summary = bart_abstractive_summary(\n",
        "    extractive_text_for_abstractive,\n",
        "    max_length=150, # Max length of the final abstractive summary\n",
        "    min_length=50,  # Min length of the final abstractive summary\n",
        "    num_beams=4     # Beam search parameter for quality\n",
        ")\n",
        "print(\"\\nAbstractive Summary:\")\n",
        "print(final_abstractive_summary)\n",
        "\n",
        "print(\"\\n--- Hybrid Summarization Pipeline Complete ---\")"
      ],
      "metadata": {
        "id": "Kzc4YCE7ADPZ",
        "outputId": "6d45b1a0-2b16-4607-a33c-b7a87f851c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Hybrid Summarization Pipeline ---\n",
            "Original Document Length (sentences): 20\n",
            "\n",
            "[Pipeline Step 1/3] Calculating document embeddings with Longformer...\n",
            "Total sentences to process: 20\n",
            "  Embeddings calculation complete.\n",
            "\n",
            "[Pipeline Step 2/3] Generating combined extractive summary...\n",
            "\n",
            "--- Starting Combined Extractive Summarization Candidate Generation ---\n",
            "  Centroid proposed 7 candidates.\n",
            "  K-Means proposed 5 candidates.\n",
            "  Total unique candidates: 11. Extracting 6 for combined summary.\n",
            "--- Combined Extractive Summarization Selection Complete ---\n",
            "  Extracted 6 sentences.\n",
            "\n",
            "Extractive Summary:\n",
            "1. Artificial intelligence (AI) has rapidly transformed various sectors, revolutionizing industries from healthcare to finance.\n",
            "2. The integration of AI into electronic health records is also streamlining administrative tasks, freeing up medical professionals to focus more on patient care.\n",
            "3. Furthermore, robo-advisors powered by AI provide automated, data-driven investment advice tailored to individual risk tolerance and financial goals, making financial planning more accessible to a wider demographic.\n",
            "4. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation.\n",
            "5. The future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence (AGI) and enhanced human-computer interaction, leading to smarter cities and more efficient resource management.\n",
            "6. The rapid pace of development means that continuous public discourse and legislative adaptation are critical to navigate the challenges and maximize the societal benefits of AI, ensuring it serves humanity's best interests.\n",
            "\n",
            "[Pipeline Step 3/3] Generating abstractive summary with BART...\n",
            "\n",
            "--- Starting BART Abstractive Summarization ---\n",
            "--- BART Abstractive Summarization Complete ---\n",
            "\n",
            "Abstractive Summary:\n",
            "Artificial intelligence (AI) has rapidly transformed various sectors. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation. Future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence.\n",
            "\n",
            "--- Hybrid Summarization Pipeline Complete ---\n"
          ]
        }
      ]
    }
  ]
}